# InstiGPT Faculty Scraper

**AI-powered universal faculty data scraper** that extracts professor profiles from any university website and enriches them with Google Scholar metrics.

## ğŸš€ Key Features

- **ğŸ” Universal Auto-Discovery**: Automatically detects faculty list pages from a university homepage or sub-page.
- **âš¡ Hybrid Extraction**: Uses fast CSS selectors for speed, falling back to LLM-based extraction for complex layouts.
- **ğŸ§  Semantic Page Analysis**: Intelligently identifies "Faculty Directories" vs "Staff" or "News" pages.
- **ğŸ“ Google Scholar Linking**: **[NEW]** Automatically searches and links faculty to their Google Scholar profiles to fetch:
  - H-Index
  - Total Citations
  - Top Paper Titles
- **ğŸ› ï¸ LLM Agnostic**: Supports **OpenAI (GPT-4o)** for quality or **Ollama** for free local inference.
- **ğŸ“¦ Batch Processing**: Process hundreds of universities from an Excel sheet.

## ğŸ› ï¸ Installation

Prerequisites: `python >= 3.10`, `uv` (recommended).

```bash
# 1. Create and activate virtual environment
uv venv .venv
source .venv/bin/activate

# 2. Install dependencies (editable mode recommended for dev)
uv pip install -e .

# 3. Setup browser for Crawl4AI
python -m playwright install chromium
```

## âš™ï¸ Configuration

Create a `.env` file or export environment variables:

### Option A: OpenAI (High Quality)
```bash
export OPENAI_API_KEY="sk-..."
```

### Option B: Ollama (Free/Local)
```bash
# Start Ollama server first
ollama run llama3.1:8b

export OLLAMA_BASE_URL="http://localhost:11434"
# The scraper will automatically detect this and switch to local models
```

## ğŸ“– Usage

### 1. Single University Scrape
Scrape a specific URL and discover faculty profiles.

```bash
# Direct scrape of a known list page
insti-scraper --url "https://nse.mit.edu/people/faculty" --output results.json

# Auto-discover faculty pages from a homepage
insti-scraper --url "https://www.stanford.edu" --discover
```

### 2. Batch Processing
Process multiple universities from an Excel file (Input columns: `Name`, `Uni faculty link`).

```bash
insti-batch --input targets.xlsx --output-dir ./results
```

**Options:**
- `--discover`: Enable auto-discovery for all links (useful if links are generic homepages).
- `--limit 5`: Process only the first 5 rows.
- `--skip-bad`: Skip URLs that look like "bad" links (e.g., login pages).

## ğŸ“Š Output Format

The scraper produces rich JSON output. Note the flat structure for Google Scholar data:

```json
[
  {
    "name": "Inder Sekhar Yadav",
    "university": "IIT Kharagpur",
    "department": "Humanities",
    "profile_url": "https://iitkgp.ac.in/department/HS/faculty/isy",
    "email": "isy@hss.iitkgp.ac.in",
    "research_interests": ["Financial Economics", "Macroeconomics"],
    
    // Google Scholar Data
    "google_scholar_url": "https://scholar.google.com/citations?user=aol7UFwAAAAJ",
    "h_index": "12",
    "total_citations": "1203",
    "paper_titles": [
      "Financial development and economic growth...",
      "The nexus between firm size, growth and profitability..."
    ]
  }
]
```

## ğŸ—ï¸ Architecture

1.  **Phase 1: Discovery**
    *   Crawls the entry URL.
    *   Uses Vision/LLM analysis to identify "directory" pages.
    *   Extracts basic profile links using generated CSS selectors.
2.  **Phase 2: Enrichment**
    *   Visits each profile page.
    *   Extracts email, detailed research interests, and bio.
3.  **Phase 3: Scholar Linking**
    *   Searches DuckDuckGo for the professor's Scholar profile.
    *   Uses LLM to verify the correct match.
    *   Scrapes metrics (H-index, Citations) directly from Scholar.

## ğŸ“‚ Project Structure

```text
instiGPT/
â”œâ”€â”€ insti_scraper/          # Main package
â”‚   â”œâ”€â”€ core/               # Config and models
â”‚   â”œâ”€â”€ scrapers/           # Scraper logic
â”‚   â”‚   â”œâ”€â”€ list_scraper.py      # Phase 1
â”‚   â”‚   â”œâ”€â”€ detail_scraper.py    # Phase 2
â”‚   â”‚   â””â”€â”€ google_scholar_scraper.py # Phase 3
â”‚   â””â”€â”€ orchestration/      # Pipeline management
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ archives/               # Old logs and data storage
â””â”€â”€ targets.xlsx            # Batch input file
```

## ğŸ“„ License
MIT
