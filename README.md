# InstiGPT Faculty Scraper

AI-powered universal faculty data scraper that extracts professor profiles from university websites.

## Features

- ğŸ” **Auto Schema Discovery** - LLM detects page structure automatically
- âš¡ **CSS + LLM Fallback** - Fast CSS extraction with LLM backup
- ğŸ“Š **Batch Processing** - Scrape multiple universities from Excel
- ğŸ“ **Detailed Logging** - Debug logs saved to `logs/`

## âš ï¸ Known Limitations

> **ğŸ”´ IMPORTANT: URL Requirements**
> 
> This scraper works best with **direct faculty/people directory pages**, NOT department landing pages.
> 
> âŒ **Won't work well:**
> - `https://university.edu/engineering/` (department homepage)
> - `https://university.edu/faculties-and-departments/` (lists faculties, not people)
> 
> âœ… **Works best with:**
> - `https://university.edu/people/` (people directory)
> - `https://university.edu/faculty/directory/` (faculty listing)
> - `https://profiles.university.edu/` (profile pages)

> **ğŸ”´ Current Shortcomings:**
> - May extract departments/faculties instead of individual professors
> - Duplicate entries possible when same URL appears multiple times
> - No pagination support for large directories yet
> - Some universities require login or have anti-scraping measures

## Installation

```bash
# Clone and install
git clone <repo-url>
cd instiGPT
make install

# Setup browser
make setup
```

## Quick Start

```bash
# Set API key
export OPENAI_API_KEY="your-key"

# Single URL
insti-scraper --url "https://university.edu/faculty" --output data.json

# Batch from Excel
insti-batch --input universities.xlsx --output-dir ./results --limit 5
```

## Excel Format

Your Excel file should have a column named `Uni faculty link` containing faculty page URLs.

| Name | Uni faculty link |
|------|------------------|
| MIT | https://web.mit.edu/faculty |
| Stanford | https://profiles.stanford.edu |

## Batch Output

The batch scraper automatically detects problematic URLs and generates separate reports:

```
batch_results/
â”œâ”€â”€ MIT_20241218_120000.json          # Individual university data
â”œâ”€â”€ Stanford_20241218_120100.json
â”œâ”€â”€ batch_summary_20241218_120200.json # Overall summary
â”œâ”€â”€ bad_links_20241218_120200.json     # ğŸ”´ Department pages (not faculty)
â””â”€â”€ warnings_20241218_120200.json      # âš ï¸ URLs needing review
```

**URL Quality Detection:**
- ğŸ”´ **Bad**: `/faculties-and-departments/`, `/engineering/`, `/medicine/` (department pages)
- âœ… **Good**: `/people/`, `/faculty/`, `/profiles/`, `/directory/` (faculty directories)

**Result Quality Assessment:**
- Detects when scraper extracts departments instead of professors
- Flags results with no emails or very few profiles

## Development

```bash
make dev      # Install with dev deps
make lint     # Run linter
make test     # Run tests
make clean    # Remove artifacts
```

## License

MIT
